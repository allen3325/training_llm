# training_llm

This is my playground for training LLM.

### Here to checkout my presentation about fine-tuning llm with LoRA and QLoRA
[link](https://docs.google.com/presentation/d/1u0Ec_L3QV16qSPJO0s8zAVbiKg4w0ijE6crreJw27hU/edit?usp=sharing)

### Using LoRA and QLoRA for training

| Model Name      | Fine-Tuning Details                |
|-----------------|------------------------------------|
| [Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)         | Fine-tuned with dataset [gbharti/finance-alpaca](https://huggingface.co/datasets/gbharti/finance-alpaca)         |
| [Breeze-7B-Instruct-v1_0](https://huggingface.co/MediaTek-Research/Breeze-7B-Instruct-v1_0)         | Fine-tuned with dataset [csitfun/LogiCoT](https://huggingface.co/datasets/csitfun/LogiCoT)         |
| [Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)         | Fine-tuned with dataset [csitfun/LogiCoT](https://huggingface.co/datasets/csitfun/LogiCoT)         |
